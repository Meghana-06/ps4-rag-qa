# PS-4: Retrieval Augmented Generation (RAG) based Question & Answering System

A production-ready starter repo for a **RAG-based Q&A** system using **FastAPI** + **FAISS** + **Sentence-Transformers** + **FLANâ€‘T5** (Hugging Face).
It ingests local documents, builds an embedding index, retrieves top passages, and generates grounded answers with citations.

---

## âœ¨ Features
- ğŸ“„ Ingest plain text/markdown/PDF (via `pypdf`) from `./data`
- ğŸ” FAISS vector store with `all-MiniLM-L6-v2` embeddings
- ğŸ§  Generator: `google/flan-t5-base` (works on CPU)
- ğŸ§ª Minimal tests
- ğŸš€ FastAPI endpoints: `/health`, `/ingest`, `/ask`
- ğŸ³ Dockerfile + Makefile for one-command run
- ğŸ” `.env` driven configuration

---

## ğŸ—‚ï¸ Project Structure
```
ps4-rag-qa/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py                # FastAPI app (ingest & ask)
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ config.py              # Settings
â”‚   â”œâ”€â”€ ingest.py              # Build FAISS index from ./data
â”‚   â”œâ”€â”€ retriever.py           # Retrieve top-k chunks
â”‚   â”œâ”€â”€ generator.py           # HF text-generation (FLANâ€‘T5)
â”‚   â””â”€â”€ rag_pipeline.py        # Orchestrates retrieve â†’ generate
â”œâ”€â”€ data/                      # Put your docs here
â”‚   â””â”€â”€ sample.txt
â”œâ”€â”€ artifacts/                 # Saved FAISS index + metadata
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_rag.py
â”œâ”€â”€ .vscode/settings.json      # Dev UX helpers
â”œâ”€â”€ .env.example               # Copy to .env and adjust
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup (Local)

```bash
# 1) Python env
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) (Optional) Create .env from template
cp .env.example .env

# 4) Ingest your data (reads ./data)
python -m rag.ingest

# 5) Run the API
uvicorn app.main:app --reload --port 8000
```

Open http://localhost:8000/docs for Swagger UI.

---

## ğŸ³ Run with Docker

```bash
# Build
docker build -t ps4-rag-qa .

# Ingest (mount your local data dir)
docker run --rm -v "$PWD/data":/app/data -v "$PWD/artifacts":/app/artifacts ps4-rag-qa python -m rag.ingest

# Serve API
docker run --rm -p 8000:8000 -v "$PWD/artifacts":/app/artifacts ps4-rag-qa
```

Or use `docker-compose up --build` for auto-ingest + serve (see compose file).

---

## ğŸ”Œ API

### `GET /health`
- Returns basic health info.

### `POST /ingest`
- Rebuilds the FAISS index from files in `./data`.
- Body: none

### `POST /ask`
- Body:
```json
{ "question": "What is RAG?", "top_k": 4 }
```
- Returns generated answer + retrieved chunks (citations).

---

## ğŸ§ª Tests
```bash
pytest -q
```

---

## ğŸ”§ Config (.env)
- `EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2`
- `GEN_MODEL=google/flan-t5-base`
- `CHUNK_SIZE=750`
- `CHUNK_OVERLAP=120`
- `TOP_K=4`

---

## ğŸ“Œ Notes
- FLANâ€‘T5 is a seq2seq model; for larger contexts, switch to a chat LLM (e.g., `mistralai/Mistral-7B-Instruct`) and use a lightweight API or local model.
- For PDFs, ensure `pypdf` is installed and your documents are text-extractable (scanned PDFs require OCR like `pytesseract`).

---

## ğŸ“„ License
MIT
